{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "from test_bad_word import *\n",
    "from parser_function import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(DATA_DIR + '/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_train['length']=df_train['Comment'].map(lambda x:len(x.split()))\n",
    "df_train = df_train[df_train['length']<300]\n",
    "df_test = pd.read_csv(DATA_DIR + '/test_with_solutions.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "num_train = df_train.shape[0]\n",
    "num_test = df_test.shape[0]\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train_1 = pd.read_csv(DATA_DIR + '/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_train_2 = pd.read_csv(DATA_DIR + '/test_with_solutions.csv', encoding=\"ISO-8859-1\")\n",
    "df_train = pd.concat((df_train_1, df_train_2), axis=0, ignore_index=True)\n",
    "df_train['length']=df_train['Comment'].map(lambda x:len(x.split()))\n",
    "f_train = df_train[df_train['length']<300]\n",
    "df_test = pd.read_csv(DATA_DIR + '/impermium_verification_labels.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "num_train = df_train.shape[0]\n",
    "num_test = df_test.shape[0]\n",
    "\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['Comment']=df_all['Comment'].map(lambda x:parser(x))\n",
    "df_all['Comment']=df_all['Comment'].map(lambda x:badword_replacer(x))\n",
    "\n",
    "#df_all['Comment']=df_all['Comment'].map(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['length']=df_all['Comment'].map(lambda x:len(x.split()))\n",
    "length = sparse.csr_matrix(df_all['length'].values).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* counts/ratios of bad/strong positive/strong negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_word_1  = [line.rstrip('\\n') for line in open('google_bad_word.txt')]\n",
    "#bad_word_2  = [line.rstrip('\\n') for line in open('handcrafted_badword.txt')]\n",
    "bad_word= set(bad_word_1  + test_bad_word) \n",
    "\n",
    "df_all['bad word count']=df_all['Comment'].map(lambda x:sum([word.lower() in bad_word for word in x.split()]))\n",
    "df_all['bad word ratio']=df_all['bad word count']/df_all['length']\n",
    "\n",
    "bad_word_count = sparse.csr_matrix(df_all['bad word count'].values).T\n",
    "bad_word_ratio = sparse.csr_matrix(df_all['bad word ratio'].values).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strong_pos  = [line.rstrip('\\n') for line in open('strong_pos.txt')]\n",
    "strong_neg  = [line.rstrip('\\n') for line in open('strong_neg.txt')]\n",
    "weak_pos  = [line.rstrip('\\n') for line in open('weak_pos.txt')]\n",
    "weak_neg  = [line.rstrip('\\n') for line in open('weak_neg.txt')]\n",
    "\n",
    "df_all['strong pos count']=df_all['Comment'].map(lambda x:sum([word.lower() in strong_pos for word in x.split()]))\n",
    "df_all['strong pos ratio']=df_all['strong pos count']/df_all['length']\n",
    "df_all['strong neg count']=df_all['Comment'].map(lambda x:sum([word.lower() in strong_neg for word in x.split()]))\n",
    "df_all['strong neg ratio']=df_all['strong neg count']/df_all['length']\n",
    "\n",
    "df_all['weak pos count']=df_all['Comment'].map(lambda x:sum([word.lower() in weak_pos for word in x.split()]))\n",
    "df_all['weak neg count']=df_all['Comment'].map(lambda x:sum([word.lower() in weak_neg for word in x.split()]))\n",
    "\n",
    "strong_pos_count = sparse.csr_matrix(df_all['strong pos count'].values).T\n",
    "strong_pos_ratio = sparse.csr_matrix(df_all['strong pos ratio'].values).T\n",
    "strong_neg_count = sparse.csr_matrix(df_all['strong neg count'].values).T\n",
    "strong_neg_ratio = sparse.csr_matrix(df_all['strong neg ratio'].values).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['sentence score']= np.exp((-3*df_all['bad word count'] + (-2)*df_all['strong neg count']+ (-1)*df_all['weak neg count']\\\n",
    "                        + 1*df_all['weak pos count'] + 2 * df_all['strong pos count'])/df_all['length'])\n",
    " \n",
    "sentence_score = sparse.csr_matrix(df_all['sentence score'].values).T    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* other binary/count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all['capital count']=df_all['Comment'].map(lambda x:sum([1 if word.isupper() else 0 for word in x.split()]))\n",
    "df_all['capital ratio']=df_all['capital count']/df_all['length']\n",
    "\n",
    "df_all['average word length']=df_all['Comment'].map(lambda x: np.mean([len(word) for word in x.split()]))\n",
    "df_all['max word length']=df_all['Comment'].map(lambda x: np.max([len(word) for word in x.split()]))\n",
    "\n",
    "df_all['email']=df_all['Comment'].map(lambda x: np.sum([1 if word=='_email_' else 0 for word in x.split()]))\n",
    "df_all['hashtag']=df_all['Comment'].map(lambda x: np.sum([1 if word=='_hashtag_' else 0 for word in x.split()]))\n",
    "df_all['url']=df_all['Comment'].map(lambda x: np.sum([1 if word=='_url_' else 0 for word in x.split()]))\n",
    "df_all['CR']=df_all['Comment'].map(lambda x: np.sum([1 if word=='_CR_' else 0 for word in x.split()]))\n",
    "\n",
    "def youare_count(x):\n",
    "    if re.search('you are',x.lower()):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_all['you are']=df_all['Comment'].map(lambda x: youare_count(x))\n",
    "\n",
    "capital_count = sparse.csr_matrix(df_all['capital count'].values).T\n",
    "capital_ratio = sparse.csr_matrix(df_all['capital ratio'].values).T\n",
    "average_word_length = sparse.csr_matrix(df_all['average word length'].values).T\n",
    "max_word_length = sparse.csr_matrix(df_all['max word length'].values).T\n",
    "email = sparse.csr_matrix(df_all['email'].values).T\n",
    "hashtag = sparse.csr_matrix(df_all['hashtag'].values).T\n",
    "url = sparse.csr_matrix(df_all['url'].values).T\n",
    "CR = sparse.csr_matrix(df_all['CR'].values).T\n",
    "you_are = sparse.csr_matrix(df_all['you are'].values).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "features.append(length)\n",
    "features.append(bad_word_count)\n",
    "features.append(bad_word_ratio)\n",
    "\n",
    "#features.append(strong_pos_count)\n",
    "features.append(strong_pos_ratio)\n",
    "#features.append(strong_neg_count)\n",
    "features.append(strong_neg_ratio)\n",
    "\n",
    "features.append(sentence_score)\n",
    "\n",
    "#features.append(capital_count) \n",
    "features.append(capital_ratio) \n",
    "features.append(average_word_length) \n",
    "features.append(max_word_length)\n",
    "features.append(email) \n",
    "features.append(hashtag) \n",
    "features.append(url)\n",
    "features.append(CR) \n",
    "features.append(you_are)\n",
    "\n",
    "features = sparse.hstack(features).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Train/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = features[:num_train]\n",
    "X_test = features[num_train:]\n",
    "\n",
    "y_train = df_train['Insult'].values\n",
    "y_test = df_test['Insult'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "ch2 = SelectKBest(chi2, k=12)\n",
    "X_train = ch2.fit_transform(X_train, y_train)\n",
    "X_test = ch2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model training time:', 0.0, 'minutes\\n')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "t_1 = time.time()\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=2500, max_depth=15, random_state=2017)\n",
    "clf = LogisticRegression(tol=1e-8, penalty='l2', C=1.5, class_weight = 'balanced')\n",
    "#rf = RandomForestRegressor(n_estimators=250, max_depth=15, random_state=0)\n",
    "#clf = BaggingRegressor(rf, n_estimators=145, max_samples=0.1, random_state=25)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "'model training time:',round((time.time()-t_1)/60,1) ,'minutes\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86200246358895283"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = LogisticRegression(tol=1e-8, penalty='l2', C=200)\n",
    "selector = RFE(estimator, 5, step=1)\n",
    "selector = selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model training time:', 0.0, 'minutes\\n')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "t_1 = time.time()\n",
    "\n",
    "clf = LinearSVC(C=0.1, class_weight={0:1,1:5})\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "'model training time:',round((time.time()-t_1)/60,1) ,'minutes\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76577257272383836"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred==y_test)/len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
