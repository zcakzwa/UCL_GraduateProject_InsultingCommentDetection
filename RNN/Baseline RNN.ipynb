{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "root_dir = \"../\"\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from test_bad_word import *\n",
    "from utility import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sentence = np.load('Sentence-Level-Preprocessed-Data/train_sentence.npy')\n",
    "train_length = np.load('Sentence-Level-Preprocessed-Data/train_length.npy')\n",
    "train_label = np.load('Sentence-Level-Preprocessed-Data/train_label.npy')\n",
    "\n",
    "test_sentence = np.load('Sentence-Level-Preprocessed-Data/test_sentence.npy')\n",
    "test_length = np.load('Sentence-Level-Preprocessed-Data/test_length.npy')\n",
    "test_label = np.load('Sentence-Level-Preprocessed-Data/test_label.npy')\n",
    "\n",
    "ver_sentence = np.load('Sentence-Level-Preprocessed-Data/val_sentence.npy')\n",
    "ver_length = np.load('Sentence-Level-Preprocessed-Data/val_length.npy')\n",
    "ver_label = np.load('Sentence-Level-Preprocessed-Data/val_label.npy')\n",
    "\n",
    "embedding = np.load('Sentence-Level-Preprocessed-Data/embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slice_index = 200\n",
    "\n",
    "train_sentence = train_sentence[:,:slice_index]\n",
    "test_sentence = test_sentence[:,:slice_index]\n",
    "ver_sentence = ver_sentence[:,:slice_index]\n",
    "\n",
    "train_length = np.array([i if i <= slice_index else slice_index for i in train_length])\n",
    "test_length = np.array([i if i <= slice_index else slice_index for i in test_length])\n",
    "ver_length = np.array([i if i <= slice_index else slice_index for i in ver_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence  = np.concatenate((train_sentence, test_sentence))\n",
    "length = np.concatenate((train_length, test_length))\n",
    "label = np.concatenate((train_label, test_label))\n",
    "\n",
    "\n",
    "np.random.seed(10)\n",
    "val_list = np.random.choice(range(len(sentence)), round(len(sentence)/10))\n",
    "train_list = list(set(range(len(sentence))).difference(set(val_list))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sentence = sentence[train_list]\n",
    "train_length = length[train_list]\n",
    "train_label = label[train_list]\n",
    "\n",
    "val_sentence = sentence[val_list]\n",
    "val_length = length[val_list]\n",
    "val_label = label[val_list]\n",
    "\n",
    "test_sentence = ver_sentence\n",
    "test_length = ver_length\n",
    "test_label = ver_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 3</font>: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def attention(inputs, max_len, attention_size, time_major=False):\n",
    "  \n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    inputs_shape = inputs.shape\n",
    "    sequence_length = max_len  # the length of sequences processed in the antecedent RNN layer\n",
    "    hidden_size = inputs_shape[2].value  # hidden size of the RNN layer\n",
    "\n",
    "    # Attention mechanism\n",
    "    W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.2, seed =1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1, seed =1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.2, seed =1))\n",
    "\n",
    "    v = tf.tanh(tf.matmul(tf.reshape(inputs, [-1, hidden_size]), W_omega) + tf.reshape(b_omega, [1, -1]))\n",
    "    vu = tf.matmul(v, tf.reshape(u_omega, [-1, 1]))\n",
    "    exps = tf.reshape(tf.exp(vu), [-1, sequence_length])\n",
    "    alphas = exps / tf.reshape(tf.reduce_sum(exps, 1), [-1, 1])\n",
    "\n",
    "    # Output of Bi-RNN is reduced with attention vector\n",
    "    output = tf.reduce_sum(inputs * tf.reshape(alphas, [-1, sequence_length, 1]), 1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = embedding.shape[0]\n",
    "word2vec_size = embedding.shape[1]\n",
    "max_len = train_sentence.shape[1]\n",
    "\n",
    "batch_size = 200\n",
    "RNN_hidden_size = 100\n",
    "fully_connected_hidden_size = 512\n",
    "multi_layer_size = 1\n",
    "target_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph created\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sentence = tf.placeholder(tf.int32, [None, None], \"sentence\")               \n",
    "sentence_length = tf.placeholder(tf.int32, [None], \"sentence_length\")        \n",
    "label = tf.placeholder(tf.int64, [None], \"label\")                         \n",
    "keep_prob = tf.placeholder(tf.float32, (),  'keep_prob')\n",
    "max_len = tf.placeholder(tf.int32, (),  'max_len')\n",
    "\n",
    "initializer = tf.random_uniform_initializer(-0.5, 0.5)\n",
    "with tf.device('/cpu:0'):\n",
    "    embeddings = tf.get_variable(\"embeddings\", [vocab_size, word2vec_size], initializer=initializer, trainable= True)\n",
    "    embeddings = embeddings.assign(embedding)\n",
    "\n",
    "sentence_embedded = tf.nn.embedding_lookup(embeddings, sentence)  \n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- sentence encoders -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "\n",
    "sentence_cell = tf.contrib.rnn.LSTMCell(RNN_hidden_size) \n",
    "sentence_all, sentence_final_state  = tf.nn.dynamic_rnn(sentence_cell,\\\n",
    "                                        sentence_embedded, sequence_length = sentence_length, dtype=tf.float32)        \n",
    "   \n",
    "#sentence_vector = tf.nn.relu(sentence_final_state.h)\n",
    "h = tf.layers.batch_normalization(sentence_final_state.h)\n",
    "h = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- loss & prediction -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ###     \n",
    "\n",
    "for i in range(multi_layer_size):\n",
    "    h = tf.contrib.layers.linear(h, 100, activation_fn = tf.nn.relu)\n",
    "    #h = tf.layers.batch_normalization(h)\n",
    "\n",
    "logits = tf.contrib.layers.linear(h, target_size)\n",
    "\n",
    "probability = tf.nn.softmax(logits)\n",
    "predict = tf.argmax(probability, axis=1)\n",
    "\n",
    "#L2 = tf.add_n([ tf.nn.l2_loss(v) for v in tf.trainable_variables()]) \n",
    "loss = tf.reduce_sum(tf.losses.sparse_softmax_cross_entropy(logits = logits, labels = label, weights = label*2 + 1 ))\n",
    "opt_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "print('graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Epoch 1 -----\n",
      " Train loss: 0.00867553151294 Time: 0.35 minute\n",
      " Train F1: 0.641833810888  Train AUC: 0.882906036482\n",
      " Dev F1: 0.643326039387  Dev AUC: 0.875235271974\n",
      "----- Epoch 2 -----\n",
      " Train loss: 0.00713571501558 Time: 0.33 minute\n",
      " Train F1: 0.659217877095  Train AUC: 0.902818696588\n",
      " Dev F1: 0.681614349776  Dev AUC: 0.892692452475\n",
      "----- Epoch 3 -----\n",
      " Train loss: 0.00632639596326 Time: 0.29 minute\n",
      " Train F1: 0.701219512195  Train AUC: 0.909337846591\n",
      " Dev F1: 0.686774941995  Dev AUC: 0.9025738754\n",
      "----- Epoch 4 -----\n",
      " Train loss: 0.0063534771886 Time: 0.27 minute\n",
      " Train F1: 0.703812316716  Train AUC: 0.918385256692\n",
      " Dev F1: 0.692840646651  Dev AUC: 0.902762092979\n",
      "----- Epoch 5 -----\n",
      " Train loss: 0.00583690945684 Time: 0.3 minute\n",
      " Train F1: 0.678062678063  Train AUC: 0.919659834096\n",
      " Dev F1: 0.687640449438  Dev AUC: 0.898691887822\n",
      "----- Epoch 6 -----\n",
      " Train loss: 0.00593576674793 Time: 0.33 minute\n",
      " Train F1: 0.75  Train AUC: 0.931005662467\n",
      " Dev F1: 0.700729927007  Dev AUC: 0.909655561829\n",
      "----- Epoch 7 -----\n",
      " Train loss: 0.00544085840401 Time: 0.23 minute\n",
      " Train F1: 0.763406940063  Train AUC: 0.938235232663\n",
      " Dev F1: 0.718826405868  Dev AUC: 0.912731742895\n",
      "----- Epoch 8 -----\n",
      " Train loss: 0.0053310187746 Time: 0.28 minute\n",
      " Train F1: 0.752351097179  Train AUC: 0.936417392758\n",
      " Dev F1: 0.709832134293  Dev AUC: 0.90744400527\n",
      "----- Epoch 9 -----\n",
      " Train loss: 0.00521844964999 Time: 0.25 minute\n",
      " Train F1: 0.738095238095  Train AUC: 0.943876804781\n",
      " Dev F1: 0.723004694836  Dev AUC: 0.909973178995\n",
      "----- Epoch 10 -----\n",
      " Train loss: 0.0047865113906 Time: 0.23 minute\n",
      " Train F1: 0.738738738739  Train AUC: 0.94400217305\n",
      " Dev F1: 0.718894009217  Dev AUC: 0.913749294184\n",
      "----- Epoch 11 -----\n",
      " Train loss: 0.00496841677083 Time: 0.25 minute\n",
      " Train F1: 0.797297297297  Train AUC: 0.956204684594\n",
      " Dev F1: 0.734375  Dev AUC: 0.90836156597\n",
      "----- Epoch 12 -----\n",
      " Train loss: 0.00459333590713 Time: 0.28 minute\n",
      " Train F1: 0.784810126582  Train AUC: 0.959903048538\n",
      " Dev F1: 0.741293532338  Dev AUC: 0.917337191794\n",
      "----- Epoch 13 -----\n",
      " Train loss: 0.00451685449471 Time: 0.27 minute\n",
      " Train F1: 0.775  Train AUC: 0.962138782674\n",
      " Dev F1: 0.731234866828  Dev AUC: 0.915325616413\n",
      "----- Epoch 14 -----\n",
      " Train loss: 0.00467180603953 Time: 0.25 minute\n",
      " Train F1: 0.725212464589  Train AUC: 0.964228253829\n",
      " Dev F1: 0.68669527897  Dev AUC: 0.916349049501\n",
      "----- Epoch 15 -----\n",
      " Train loss: 0.00430730947812 Time: 0.26 minute\n",
      " Train F1: 0.775  Train AUC: 0.965168515849\n",
      " Dev F1: 0.733644859813  Dev AUC: 0.921207415773\n",
      "----- Epoch 16 -----\n",
      " Train loss: 0.0040957890976 Time: 0.24 minute\n",
      " Train F1: 0.834482758621  Train AUC: 0.971959297102\n",
      " Dev F1: 0.755208333333  Dev AUC: 0.921513269339\n",
      "----- Epoch 17 -----\n",
      " Train loss: 0.00389343157111 Time: 0.23 minute\n",
      " Train F1: 0.808777429467  Train AUC: 0.978081447586\n",
      " Dev F1: 0.717592592593  Dev AUC: 0.920936853002\n",
      "----- Epoch 18 -----\n",
      " Train loss: 0.00386204065492 Time: 0.23 minute\n",
      " Train F1: 0.841750841751  Train AUC: 0.974989030276\n",
      " Dev F1: 0.744303797468  Dev AUC: 0.924113024657\n",
      "----- Epoch 19 -----\n",
      " Train loss: 0.00362564484845 Time: 0.24 minute\n",
      " Train F1: 0.848484848485  Train AUC: 0.978060552874\n",
      " Dev F1: 0.739583333333  Dev AUC: 0.9181959345\n",
      "----- Epoch 20 -----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-66cc28e61599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst_train_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minst_train_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m                    \u001b[0mmax_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_sentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mopt_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\zcakz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "split_num = 5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n = train_sentence.shape[0]\n",
    "\n",
    "    part_train_sentence = train_sentence[:500]\n",
    "    part_train_length = train_length[:500]\n",
    "    part_train_label = train_label[:500]\n",
    "    train_feed_dict = {sentence: part_train_sentence, sentence_length: part_train_length, label: part_train_label, keep_prob: 1, \\\n",
    "                       max_len: train_sentence.shape[1]}\n",
    "    \n",
    "    val_sentence_splited = list(split(val_sentence, split_num))\n",
    "    val_label_splited = list(split(val_label, split_num))\n",
    "    val_length_splited = list(split(val_length, split_num))    \n",
    "\n",
    "    for epoch in range(25):\n",
    "        print('----- Epoch', epoch + 1, '-----')\n",
    "        total_loss = 0\n",
    "        t_0 = time.time()\n",
    "        \n",
    "        train_index = list(range(len(train_sentence)))\n",
    "        random.shuffle(train_index)\n",
    "        random_train_index = list(chunks(train_index, n // BATCH_SIZE))\n",
    "        \n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            inst_train_sentence = train_sentence[random_train_index[i]]\n",
    "            inst_train_label = train_label[random_train_index[i]]\n",
    "            inst_train_length = train_length[random_train_index[i]]\n",
    "\n",
    "            feed_dict = {sentence: inst_train_sentence, sentence_length: inst_train_length, label:inst_train_label, keep_prob: 0.75,\\\n",
    "                    max_len: train_sentence.shape[1]}\n",
    "            _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "            total_loss += current_loss\n",
    "\n",
    "        print(' Train loss:', total_loss / n, 'Time:', round((time.time()-t_0)/60,2),'minute') \n",
    "        save_path = saver.save(sess, \"./rnn_baseline_model_%s.ckpt\" % (epoch+1))\n",
    "        \n",
    "        train_predicted, train_probability = sess.run([predict, probability], feed_dict=train_feed_dict)\n",
    "        train_f1 = f1_score(train_label[:500], train_predicted)\n",
    "        train_auc = roc_auc_score(train_label[:500], np.array(train_probability)[:,1])\n",
    "        print(' Train F1:', train_f1,' Train AUC:', train_auc)\n",
    "        \n",
    "        val_predicted = []\n",
    "        val_probability = []\n",
    "        for i in range(split_num):\n",
    "            val_feed_dict = {sentence: val_sentence_splited[i], label: val_label_splited[i], sentence_length: val_length_splited[i], \\\n",
    "                              keep_prob: 1, max_len: val_sentence.shape[1]} \n",
    "            val_current_predicted, val_current_probability = sess.run([predict,probability], feed_dict=val_feed_dict)\n",
    "            val_predicted = val_predicted + list(val_current_predicted)\n",
    "            val_probability = val_probability + list(val_current_probability)\n",
    "\n",
    "        val_f1 = f1_score(val_label, val_predicted)\n",
    "        val_auc = roc_auc_score(val_label, np.array(val_probability)[:,1])\n",
    "        print(' Dev F1:', val_f1,' Dev AUC:', val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnn_baseline_model_16.ckpt\n",
      " Dev F1: 0.755208333333  Dev AUC: 0.921513269339\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "split_num = 5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"rnn_baseline_model_16.ckpt\")\n",
    "    n = train_sentence.shape[0]\n",
    "\n",
    "    val_sentence_splited = list(split(val_sentence, split_num))\n",
    "    val_label_splited = list(split(val_label, split_num))\n",
    "    val_length_splited = list(split(val_length, split_num))    \n",
    "\n",
    "    val_predicted = []\n",
    "    val_probability = []\n",
    "    for i in range(split_num):\n",
    "        val_feed_dict = {sentence: val_sentence_splited[i], label: val_label_splited[i], sentence_length: val_length_splited[i], \\\n",
    "                          keep_prob: 1, max_len: val_sentence.shape[1]} \n",
    "        val_current_predicted, val_current_probability = sess.run([predict,probability], feed_dict=val_feed_dict)\n",
    "        val_predicted = val_predicted + list(val_current_predicted)\n",
    "        val_probability = val_probability + list(val_current_probability)\n",
    "\n",
    "    val_f1 = f1_score(val_label, val_predicted)\n",
    "    val_auc = roc_auc_score(val_label, np.array(val_probability)[:,1])\n",
    "    print(' Dev F1:', val_f1,' Dev AUC:', val_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnn_baseline_model_16.ckpt\n",
      " Dev F1: 0.764705882353  Dev AUC: 0.839627202794\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "split_num = 5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"rnn_baseline_model_16.ckpt\")\n",
    "    n = train_sentence.shape[0]\n",
    "\n",
    "    test_sentence_splited = list(split(test_sentence, split_num))\n",
    "    test_label_splited = list(split(test_label, split_num))\n",
    "    test_length_splited = list(split(test_length, split_num))    \n",
    "\n",
    "    test_predicted = []\n",
    "    test_probability = []\n",
    "    for i in range(split_num):\n",
    "        test_feed_dict = {sentence: test_sentence_splited[i], label: test_label_splited[i], sentence_length: test_length_splited[i], \\\n",
    "                          keep_prob: 1, max_len: test_sentence.shape[1]} \n",
    "        test_current_predicted, test_current_probability = sess.run([predict,probability], feed_dict=test_feed_dict)\n",
    "        test_predicted = test_predicted + list(test_current_predicted)\n",
    "        test_probability = test_probability + list(test_current_probability)\n",
    "\n",
    "    test_f1 = f1_score(test_label, test_predicted)\n",
    "    test_auc = roc_auc_score(test_label, np.array(test_probability)[:,1])\n",
    "    print(' Dev F1:', test_f1,' Dev AUC:', test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[954 204]\n",
      " [284 793]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAADtCAYAAADwQMNgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKmq/wMUPWf4XeNiEwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a918943f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_confusion_matrix_dict(matrix,rotation=45, outside_label=\"\"):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(0)\n",
    "    plt.xticks(tick_marks, [0,1], rotation=rotation)\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    \n",
    "cm=confusion_matrix(test_label, test_predicted)\n",
    "print(cm)\n",
    "plot_confusion_matrix_dict(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('rnn_baseline_pred_test_prob', np.array(test_probability)[:,1])\n",
    "np.save('rnn_baseline_pred_test', test_predicted)\n",
    "np.save('rnn_baseline_pred_val_prob', np.array(val_probability)[:,1])\n",
    "np.save('rnn_baseline_pred_val', val_predicted)\n",
    "np.save('val_real',val_label)\n",
    "np.save('test_real',test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
