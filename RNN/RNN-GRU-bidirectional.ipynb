{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "root_dir = \"../\"\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from test_bad_word import *\n",
    "from utility import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.options.display.mpl_style = 'default'\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import sparse\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_word_1  = [line.rstrip('\\n') for line in open('../wordlist/google_bad_word.txt')]\n",
    "#bad_word_2  = [line.rstrip('\\n') for line in open('handcrafted_badword.txt')]\n",
    "bad_word= set(bad_word_1  + test_bad_word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/RNN_train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('data/RNN_test.csv', encoding=\"ISO-8859-1\")\n",
    "df_val = pd.read_csv('data/RNN_val.csv', encoding=\"ISO-8859-1\")\n",
    "\n",
    "num_train = df_train.shape[0]\n",
    "num_test = df_test.shape[0]\n",
    "num_val = df_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_transform(file):\n",
    "    data = []\n",
    "    length = len(file)\n",
    "    sentences = file['Comment constructed'].values\n",
    "    insults = file['Insult'].values\n",
    "    for i in range(length):\n",
    "        current_sentences = sentences[i]\n",
    "        current_insult = insults[i]\n",
    "        instance = {\"sentences\": eval(current_sentences), \"insult\": current_insult}\n",
    "        data.append(instance)\n",
    "    return data\n",
    "\n",
    "train = data_transform(df_train)\n",
    "test = data_transform(df_test)\n",
    "val = data_transform(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train + test\n",
    "test = val\n",
    "num_train = num_train + num_test\n",
    "num_test = num_val\n",
    "\n",
    "df_train = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_test = df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6594"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 3</font>: Pipeline Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* comment length: number of setences in a comment\n",
    "* sentence length: number of words in a sentene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_comment_len = 25\n",
    "max_sentence_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pipeline(data, vocab = None, max_comment_len = 17, max_sentence_len_ = None):\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "    max_sentence_len = -1\n",
    "    data_sentences = []\n",
    "    data_insults = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        for sentence_id, sentence in enumerate(instance['sentences']):\n",
    "            if sentence_id <= max_comment_len:\n",
    "                sent = []\n",
    "                tokenized = sentence.split(' ')\n",
    "                for token in tokenized:\n",
    "                    token = token.lower()\n",
    "                    if not is_ext_vocab and token not in vocab:\n",
    "                        vocab[token] = len(vocab)\n",
    "                    if token not in vocab:\n",
    "                        token_id = vocab['<OOV>']\n",
    "                    else:\n",
    "                        token_id = vocab[token]\n",
    "                    sent.append(token_id)\n",
    "                if len(sent) > max_sentence_len:\n",
    "                    max_sentence_len = len(sent)\n",
    "                sents.append(sent)\n",
    "        data_sentences.append(sents)\n",
    "        data_insults.append(instance['insult'])\n",
    "        \n",
    "    if max_sentence_len_ is not None:\n",
    "        max_sentence_len = max_sentence_len_\n",
    "    out_sentences = np.full([len(data_sentences), max_comment_len, max_sentence_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            if j < max_comment_len:\n",
    "                if len(sent) <= max_sentence_len:\n",
    "                    out_sentences[i, j, 0:len(sent)] = sent\n",
    "                else:\n",
    "                    out_sentences[i, j, 0: max_sentence_len] = sent[:max_sentence_len]\n",
    "\n",
    "    return out_sentences, np.array(data_insults), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sentences, train_insults, vocab = pipeline(train, max_comment_len = max_comment_len, max_sentence_len_ = max_sentence_len)\n",
    "\n",
    "test_sentences, test_insults, vocab = pipeline(test, vocab, max_comment_len, max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6594, 25, 25), (2235, 25, 25))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences.shape, test_sentences.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import threshold\n",
    "\n",
    "def sentence_length_filler(sentence_length, max_comment_len, max_sentence_len):\n",
    "    n = sentence_length.shape[0]\n",
    "    out_array = np.full([n, max_comment_len], 0, dtype=np.int32)\n",
    "    for i,sen_len in enumerate(sentence_length):\n",
    "        sen_len = eval(sen_len)\n",
    "        out_array[i,0:len(sen_len)] = sen_len[:max_comment_len]\n",
    "    return threshold(out_array, threshmax = max_sentence_len)\n",
    "\n",
    "def comment_length_filler(original_comment_length, max_comment_len):\n",
    "    comment_length = []\n",
    "    for i in original_comment_length:\n",
    "        if i > max_comment_len:\n",
    "            comment_length.append(15)\n",
    "        else:\n",
    "            comment_length.append(i)\n",
    "    return np.array(comment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_comment_length = comment_length_filler(df_train['comment length'].values, max_comment_len)\n",
    "train_sentences_length = df_train['sentences length'].values\n",
    "train_sentences_length = sentence_length_filler(train_sentences_length, max_comment_len, max_sentence_len)\n",
    "\n",
    "test_comment_length = comment_length_filler(df_test['comment length'].values, max_comment_len)\n",
    "test_sentences_length = df_test['sentences length'].values\n",
    "test_sentences_length = sentence_length_filler(test_sentences_length, max_comment_len, max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_comment_length_index = np.ones((num_train, max_comment_len))\n",
    "test_comment_length_index = np.ones((num_test, max_comment_len))\n",
    "\n",
    "for i,index in enumerate(train_comment_length):\n",
    "    train_comment_length_index[i][index:] = 0\n",
    "    \n",
    "for i,index in enumerate(test_comment_length):\n",
    "    test_comment_length_index[i][index:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  2, ...,  1,  6, 14], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comment_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0, ...,  0,  0,  0],\n",
       "       [ 7,  9,  0, ...,  0,  0,  0],\n",
       "       [15,  0,  0, ...,  0,  0,  0],\n",
       "       ..., \n",
       "       [ 7,  0,  0, ...,  0,  0,  0],\n",
       "       [19,  6, 12, ...,  0,  0,  0],\n",
       "       [13, 25, 14, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 4</font>: Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import operator\n",
    "\n",
    "glove_size = 100\n",
    "\n",
    "word_dict= collections.defaultdict(list)\n",
    "vocab_keys = vocab.keys()\n",
    "\n",
    "#file= open('word2vec/glove.6B.%sd.txt' % glove_size, 'r', encoding='utf-8')\n",
    "file= open('word2vec/glove.twitter.27B.%sd.txt' % glove_size, 'r', encoding='utf-8')\n",
    "#file= open('word2vec/glove.840B.300d.txt', 'r', encoding='utf-8')\n",
    "for line in file:\n",
    "    line = line.rstrip().split(' ')\n",
    "    if line[0] in vocab_keys:\n",
    "        word_dict[line[0]]=[float(i) for i in line[1:]]\n",
    "    \n",
    "word_dict=dict(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_vocab = sorted(vocab.items(), key=operator.itemgetter(1))\n",
    "\n",
    "embedding_list=[]\n",
    "#OOV_vector = [random.uniform(-1, 1) for i in range(glove_size)]\n",
    "OOV_vector  = np.mean(list(word_dict.values()),axis=0)\n",
    "\n",
    "for item in sorted_vocab:\n",
    "    if item[0]== '<PAD>':\n",
    "        embedding_list.append(np.array([0 for i in range(glove_size)], dtype='f'))\n",
    "    elif item[0] =='_cr_':\n",
    "        embedding_list.append(word_dict['fuck'])\n",
    "    elif item[0] in word_dict:\n",
    "        embedding_list.append(word_dict[item[0]])\n",
    "    else:\n",
    "        if item[0] in bad_word:\n",
    "            embedding_list.append(word_dict['fuck']) \n",
    "        else:\n",
    "            embedding_list.append(OOV_vector)\n",
    "        #embedding_list.append('unseen')\n",
    "       \n",
    "W = np.array(embedding_list)\n",
    "#print(\"unseen ratio:\", embedding_list.count('unseen')/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normal_index = df_train[df_train['Insult']==0].index.tolist()\n",
    "insult_index = df_train[df_train['Insult']==1].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 5</font>: Construct RNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tricks which have positive effect:\n",
    "* pre-trained word2vec (✓,  trainable: False > True for better generalisation)\n",
    "* different max sentence/comment length ( max_comment_len: 17-->5 & max_sent_len: 50-->25 ✓)\n",
    "* recheck the pipeline (lowercase tansformation ✓)\n",
    "\n",
    "### tricks which have no obvious effect:\n",
    "* regularisation: dropout/L2 \n",
    "* weighted loss (insignificant)\n",
    "* sentence/paragraph pooling: mean pooling\n",
    "\n",
    "### tricks which have negative effect : \n",
    "* attention on sentence/word (×)\n",
    "* downsampling mini batch training (×)\n",
    "\n",
    "### to do list:\n",
    "* self trained word2vec ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MODEL PARAMETERS ###\n",
    "\n",
    "max_comment_len = train_sentences.shape[1]\n",
    "max_sen_len = train_sentences.shape[2]\n",
    "vocab_size = len(vocab)\n",
    "word2vec_size = glove_size\n",
    "sentence_hidden_size = 100\n",
    "comment_hidden_size = 100\n",
    "target_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(inputs, attention_size, time_major=False):\n",
    "\n",
    "    if isinstance(inputs, tuple):\n",
    "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
    "        inputs = tf.concat(inputs, 2)\n",
    "\n",
    "    if time_major:\n",
    "        # (T,B,D) => (B,T,D)\n",
    "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
    "\n",
    "    inputs_shape = inputs.shape\n",
    "    sequence_length = inputs_shape[1].value  # the length of sequences processed in the antecedent RNN layer\n",
    "    hidden_size = inputs_shape[2].value  # hidden size of the RNN layer\n",
    "\n",
    "    # Attention mechanism\n",
    "    W_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
    "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
    "\n",
    "    v = tf.tanh(tf.matmul(tf.reshape(inputs, [-1, hidden_size]), W_omega) + tf.reshape(b_omega, [1, -1]))\n",
    "    vu = tf.matmul(v, tf.reshape(u_omega, [-1, 1]))\n",
    "    exps = tf.multiply(tf.reshape(tf.exp(vu), [-1, sequence_length]),comment_length_index)\n",
    "    alphas = exps / tf.reshape(tf.reduce_sum(exps, 1), [-1, 1])\n",
    "\n",
    "    # Output of Bi-RNN is reduced with attention vector\n",
    "    output = tf.reduce_sum(inputs * tf.reshape(alphas, [-1, sequence_length, 1]), 1)\n",
    "    return output,alphas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph created\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sentences = tf.placeholder(tf.int64, [None, None, None], \"sentences\")                      # [batch_size x max_comment_len x max_sen_len]\n",
    "sentences_length = tf.placeholder(tf.int64, [None, None], \"sentences_length\")              # [batch_size x max_comment_len]\n",
    "comment_length = tf.placeholder(tf.int64, [None], \"comment_length\")                        # [batch_size]\n",
    "comment_length_index = tf.placeholder(tf.float32, [None, max_comment_len], \"comment_length\") # [batch_size x max_comment_len]   \n",
    "insult = tf.placeholder(tf.int64, [None], \"insult\")                                        # [batch_size]\n",
    "keep_prob = tf.placeholder(tf.float32, (),  'keep_prob')\n",
    "learning_rate = tf.placeholder(tf.float32, (),  'learning_rate')\n",
    "\n",
    "batch_size = tf.shape(sentences)[0]\n",
    "\n",
    "# max_comment_len x [batch_size x max_sen_len]\n",
    "sentences_lst = [tf.reshape(x, [batch_size, -1]) for x in tf.split(sentences, max_comment_len, 1)] \n",
    "sentences_length_lst = [tf.reshape(x,[-1]) for x in tf.split(sentences_length, max_comment_len, 1)] \n",
    "\n",
    "initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "embeddings = tf.get_variable(\"W\", [vocab_size, word2vec_size], initializer=initializer, trainable= True)\n",
    "embeddings = embeddings.assign(W) \n",
    "\n",
    "# max_comment_len x [batch_size x max_sen_len x word2vec_size]\n",
    "sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)  \n",
    "                          for sentence in sentences_lst] \n",
    "\n",
    "# split sentences_embedded into n parts, name as sentence0, sentence1, etc\n",
    "for i in range(max_comment_len):\n",
    "    globals()['sentences_embedded_%s' % i] = sentences_embedded[i]     # [batch_size x max_sen_len x word2vec_size]\n",
    "    globals()['sentences_length_%s' % i] = sentences_length_lst[i]\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- sentence encoders -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "\n",
    "lstm_sentence_cell = tf.contrib.rnn.GRUCell(sentence_hidden_size)\n",
    "lstm_sentence_cell = tf.contrib.rnn.DropoutWrapper(lstm_sentence_cell)\n",
    "with tf.variable_scope(\"sentence_encoder\") as varscope: \n",
    "    _, sentence_0 = tf.nn.dynamic_rnn(lstm_sentence_cell, sentences_embedded_0,\\\n",
    "                                                  sequence_length = sentences_length_0, dtype=tf.float32)        \n",
    "\n",
    "    for i in range(1, max_comment_len):\n",
    "        varscope.reuse_variables()\n",
    "        _, globals()['sentence_%s' % i]  = tf.nn.dynamic_rnn(lstm_sentence_cell, globals()['sentences_embedded_%s' % i],\\\n",
    "                                                    sequence_length = globals()['sentences_length_%s' % i], dtype=tf.float32) \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "sentence_vectors = tf.stack([globals()['sentence_%s' % i]   for i in range(max_comment_len)], axis=1)\n",
    "\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- comment encoders --------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "\n",
    "lstm_comment_cell_fw = tf.contrib.rnn.GRUCell(comment_hidden_size)\n",
    "lstm_comment_cell_bw = tf.contrib.rnn.GRUCell(comment_hidden_size)\n",
    "lstm_comment_cell_fw = tf.contrib.rnn.DropoutWrapper(lstm_comment_cell_fw, keep_prob, keep_prob)\n",
    "lstm_comment_cell_bw = tf.contrib.rnn.DropoutWrapper(lstm_comment_cell_bw, keep_prob, keep_prob)\n",
    "with tf.variable_scope(\"comment_encoder\") as varscope: \n",
    "    comment_vectors_all, comment_vectors = tf.nn.bidirectional_dynamic_rnn(lstm_comment_cell_fw,lstm_comment_cell_bw, \\\n",
    "                                                            sentence_vectors,sequence_length = comment_length, dtype=tf.float32)            \n",
    "\n",
    "    \n",
    "comment_all_state = tf.concat(comment_vectors_all, axis=2)\n",
    "#comment_vectors = tf.div(tf.reduce_sum(comment_all_state, 1), tf.stack([tf.cast(comment_length, tf.float32)],1))    \n",
    "#comment_vectors = tf.concat(comment_vectors, axis =1)   \n",
    " \n",
    "\n",
    "comment_vectors,alphas = attention(comment_vectors_all, 200)\n",
    "### ------------------------------------------------------------------------------------------------------- ### \n",
    "### ---------------------------------------- loss & prediction -------------------------------------------  ###\n",
    "### ------------------------------------------------------------------------------------------------------- ###     \n",
    "\n",
    "comment_vectors = tf.layers.batch_normalization(comment_vectors)\n",
    "\n",
    "h = tf.contrib.layers.linear(comment_vectors, 100, activation_fn = tf.nn.relu)\n",
    "h = tf.layers.batch_normalization(h)\n",
    "logits = tf.contrib.layers.linear(h, target_size)\n",
    "probability = tf.nn.softmax(logits)\n",
    "predict = tf.argmax(probability, axis=1)\n",
    "\n",
    "L2 = tf.add_n([ tf.nn.l2_loss(v) for v in tf.trainable_variables()]) \n",
    "\n",
    "loss = tf.reduce_sum(tf.losses.sparse_softmax_cross_entropy(logits = logits, labels = insult, weights = 1 ))\n",
    "                                                                                          ### weights = insult*2 + 1\n",
    "'''\n",
    "logits = tf.contrib.layers.fully_connected(comment_vectors, 1)\n",
    "labels = tf.cast(tf.stack([insult], axis=1), tf.float32)\n",
    "loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))    \n",
    "probability = tf.sigmoid(logits)   \n",
    "predict = tf.argmax(probability, axis=1)\n",
    "'''\n",
    "opt_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "print('graph created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameters:  max_comment_len= 25 , max_sentence_len= 25 , sentence_hidden_size= 100 , comment_hidden_size= 100 , word2vec_size= 100\n",
      "----- Epoch 1 -----\n",
      " Train loss: 0.00394458361908 Time: 0.97 minute\n",
      " Train F1: 0.750503597122  Train AUC: 0.917145438386\n",
      " Dev F1: 0.709883103082  Dev AUC: 0.80905108061\n",
      "[[1021  137]\n",
      " [ 409  668]]\n",
      "----- Epoch 2 -----\n",
      " Train loss: 0.00313132234801 Time: 0.96 minute\n",
      " Train F1: 0.741017964072  Train AUC: 0.926857484409\n",
      " Dev F1: 0.743988684583  Dev AUC: 0.823517879737\n",
      "[[903 255]\n",
      " [288 789]]\n",
      "----- Epoch 3 -----\n",
      " Train loss: 0.0028329289453 Time: 0.96 minute\n",
      " Train F1: 0.793719211823  Train AUC: 0.945047575869\n",
      " Dev F1: 0.687707641196  Dev AUC: 0.835913182367\n",
      "[[1050  108]\n",
      " [ 456  621]]\n",
      "----- Epoch 4 -----\n",
      " Train loss: 0.00263945960369 Time: 0.97 minute\n",
      " Train F1: 0.828494462736  Train AUC: 0.956180970504\n",
      " Dev F1: 0.706267029973  Dev AUC: 0.831994297471\n",
      "[[1048  110]\n",
      " [ 429  648]]\n",
      "----- Epoch 5 -----\n",
      " Train loss: 0.00228650387956 Time: 0.94 minute\n",
      " Train F1: 0.849462365591  Train AUC: 0.967475506922\n",
      " Dev F1: 0.70752688172  Dev AUC: 0.830255555395\n",
      "[[1033  125]\n",
      " [ 419  658]]\n",
      "----- Epoch 6 -----\n",
      " Train loss: 0.00196295750545 Time: 0.95 minute\n",
      " Train F1: 0.879768786127  Train AUC: 0.975998451998\n",
      " Dev F1: 0.723670490094  Dev AUC: 0.832478996381\n",
      "[[1011  147]\n",
      " [ 383  694]]\n",
      "----- Epoch 7 -----\n",
      " Train loss: 0.00175759473108 Time: 0.96 minute\n",
      " Train F1: 0.89219330855  Train AUC: 0.981418057155\n",
      " Dev F1: 0.739152628892  Dev AUC: 0.83272916356\n",
      "[[1000  158]\n",
      " [ 353  724]]\n",
      "----- Epoch 8 -----\n",
      " Train loss: 0.00155679757509 Time: 0.96 minute\n",
      " Train F1: 0.903397341211  Train AUC: 0.985787342064\n",
      " Dev F1: 0.71170212766  Dev AUC: 0.827915850817\n",
      "[[1024  134]\n",
      " [ 408  669]]\n",
      "----- Epoch 9 -----\n",
      " Train loss: 0.00137342988184 Time: 0.96 minute\n",
      " Train F1: 0.919121894858  Train AUC: 0.988610754333\n",
      " Dev F1: 0.723204994797  Dev AUC: 0.829681453792\n",
      "[[1008  150]\n",
      " [ 382  695]]\n",
      "----- Epoch 10 -----\n",
      " Train loss: 0.00119384810609 Time: 0.96 minute\n",
      " Train F1: 0.930612244898  Train AUC: 0.991491844002\n",
      " Dev F1: 0.710498409332  Dev AUC: 0.823829786893\n",
      "[[1019  139]\n",
      " [ 407  670]]\n",
      "----- Epoch 11 -----\n",
      " Train loss: 0.00106772965977 Time: 0.95 minute\n",
      " Train F1: 0.940377804014  Train AUC: 0.993538474789\n",
      " Dev F1: 0.691008174387  Dev AUC: 0.822741720028\n",
      "[[1034  124]\n",
      " [ 443  634]]\n",
      "----- Epoch 12 -----\n",
      " Train loss: 0.00105275375182 Time: 0.94 minute\n",
      " Train F1: 0.948501600233  Train AUC: 0.995378472594\n",
      " Dev F1: 0.697277095569  Dev AUC: 0.82174546131\n",
      "[[1015  143]\n",
      " [ 424  653]]\n",
      "----- Epoch 13 -----\n",
      " Train loss: 0.000859413675914 Time: 0.94 minute\n",
      " Train F1: 0.954887218045  Train AUC: 0.996103373992\n",
      " Dev F1: 0.72481827622  Dev AUC: 0.821944312144\n",
      "[[1007  151]\n",
      " [ 379  698]]\n",
      "----- Epoch 14 -----\n",
      " Train loss: 0.000795315561857 Time: 0.95 minute\n",
      " Train F1: 0.955421340236  Train AUC: 0.996787930788\n",
      " Dev F1: 0.724505327245  Dev AUC: 0.819208108624\n",
      "[[978 180]\n",
      " [363 714]]\n",
      "----- Epoch 15 -----\n",
      " Train loss: 0.00072927043025 Time: 0.95 minute\n",
      " Train F1: 0.959953903774  Train AUC: 0.997195162812\n",
      " Dev F1: 0.712962962963  Dev AUC: 0.821661270432\n",
      "[[984 174]\n",
      " [384 693]]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "split_num = 50\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    n = train_sentences.shape[0]\n",
    "    print('hyperparameters: ', 'max_comment_len=', max_comment_len, ', max_sentence_len=', max_sentence_len, \\\n",
    "          ', sentence_hidden_size=',sentence_hidden_size, ', comment_hidden_size=', comment_hidden_size , \\\n",
    "          ', word2vec_size=', word2vec_size)\n",
    "    \n",
    "    train_sentences_splited = list(split(train_sentences, split_num))\n",
    "    train_insults_splited = list(split(train_insults, split_num))\n",
    "    train_comment_length_splited = list(split(train_comment_length, split_num))\n",
    "    train_comment_length_index_splited = list(split(train_comment_length_index, split_num))\n",
    "    train_sentences_length_splited = list(split(train_sentences_length, split_num))\n",
    "    \n",
    "    test_sentences_splited = list(split(test_sentences, split_num))\n",
    "    test_insults_splited = list(split(test_insults, split_num))\n",
    "    test_comment_length_splited = list(split(test_comment_length, split_num))\n",
    "    test_comment_length_index_splited = list(split(test_comment_length_index, split_num))\n",
    "    test_sentences_length_splited = list(split(test_sentences_length, split_num))\n",
    "    for epoch in range(15):\n",
    "        print('----- Epoch', epoch + 1, '-----')\n",
    "        total_loss = 0\n",
    "        learning_rate_ = 0.005 if epoch < 5 else 0.001\n",
    "        t_0 = time.time()\n",
    "            \n",
    "        for i in range(n // BATCH_SIZE):\n",
    "            index_list = random.sample(range(n), BATCH_SIZE )\n",
    "            inst_train_sentences = [train_sentences[idx] for idx in index_list]\n",
    "            inst_train_insults = [train_insults[idx] for idx in index_list]\n",
    "            inst_train_comment_length = [train_comment_length[idx] for idx in index_list]\n",
    "            inst_train_comment_length_index = [train_comment_length_index[idx] for idx in index_list]\n",
    "            inst_train_sentences_length = [train_sentences_length[idx] for idx in index_list]\n",
    "\n",
    "            feed_dict = {sentences: inst_train_sentences, insult:inst_train_insults, \\\n",
    "                         comment_length: inst_train_comment_length, sentences_length: inst_train_sentences_length, keep_prob: 0.5,\\\n",
    "                         comment_length_index: inst_train_comment_length_index, learning_rate: learning_rate_ }\n",
    "            \n",
    "            \n",
    "            _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "            total_loss += current_loss\n",
    "     \n",
    "        print(' Train loss:', total_loss / n, 'Time:', round((time.time()-t_0)/60,2),'minute') \n",
    "\n",
    "        train_predicted = []\n",
    "        train_probability = []\n",
    "        for i in range(split_num):\n",
    "            train_feed_dict = {sentences: train_sentences_splited[i], insult: train_insults_splited[i],\\\n",
    "                               comment_length: train_comment_length_splited[i], sentences_length: train_sentences_length_splited[i],\\\n",
    "                               keep_prob: 1, comment_length_index: train_comment_length_index_splited[i]} \n",
    "            train_current_predicted, train_current_probability = sess.run([predict,probability], feed_dict=train_feed_dict)\n",
    "            train_predicted = train_predicted + list(train_current_predicted)\n",
    "            train_probability = train_probability + list(train_current_probability)\n",
    "\n",
    "        train_f1 = f1_score(train_insults, train_predicted)\n",
    "        train_auc = roc_auc_score(train_insults, np.array(train_probability)[:,1])\n",
    "        print(' Train F1:', train_f1,' Train AUC:', train_auc)   \n",
    "\n",
    "        test_predicted = []\n",
    "        test_probability = []\n",
    "        A = []\n",
    "        for i in range(split_num):\n",
    "            test_feed_dict = {sentences: test_sentences_splited[i], insult: test_insults_splited[i],\\\n",
    "                               comment_length: test_comment_length_splited[i], sentences_length: test_sentences_length_splited[i],\\\n",
    "                             keep_prob: 1, comment_length_index: test_comment_length_index_splited[i]} \n",
    "            test_current_predicted, test_current_probability = sess.run([predict,probability], feed_dict=test_feed_dict)\n",
    "            a = sess.run(alphas, feed_dict=test_feed_dict)\n",
    "            test_predicted = test_predicted + list(test_current_predicted)\n",
    "            test_probability = test_probability + list(test_current_probability)\n",
    "            A = A + list(a)\n",
    "\n",
    "        test_f1 = f1_score(test_insults, test_predicted)\n",
    "        test_auc = roc_auc_score(test_insults, np.array(test_probability)[:,1])\n",
    "        print(' Dev F1:', test_f1,' Dev AUC:', test_auc)\n",
    "        print(confusion_matrix(test_insults, test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_dict(matrix,rotation=45, outside_label=\"\"):\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(0)\n",
    "    plt.xticks(tick_marks, [0,1], rotation=rotation)\n",
    "    plt.yticks(tick_marks, [0,1])\n",
    "    \n",
    "cm=confusion_matrix(test_insults, test_predicted)\n",
    "print(cm)\n",
    "plot_confusion_matrix_dict(cm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## downsampling minibatch training\n",
    "\n",
    "    for i in range(n // BATCH_SIZE):\n",
    "                normal_index_list = random.sample(normal_index, round(BATCH_SIZE/2))\n",
    "                insult_index_list = random.sample(insult_index, round(BATCH_SIZE/2)) \n",
    "                inst_train_sentences = [train_sentences[idx] for idx in normal_index_list] \\\n",
    "                                     + [train_sentences[idx] for idx in insult_index_list]\n",
    "                inst_train_insults = [train_insults[idx] for idx in normal_index_list] \\\n",
    "                                   + [train_insults[idx] for idx in insult_index_list]\n",
    "                inst_train_comment_length = [train_comment_length[idx] for idx in normal_index_list] \\\n",
    "                                          + [train_comment_length[idx] for idx in insult_index_list]\n",
    "                inst_train_sentences_length = [train_sentences_length[idx] for idx in normal_index_list] \\\n",
    "                                            + [train_sentences_length[idx] for idx in insult_index_list] \n",
    "                \n",
    "                feed_dict = {sentences: inst_train_sentences, insult:inst_train_insults,\\\n",
    "                       comment_length: inst_train_comment_length, sentences_length: inst_train_sentences_length}\n",
    "                _, current_loss = sess.run([opt_op, loss], feed_dict=feed_dict)\n",
    "                total_loss += current_loss\n",
    "            \n",
    "            print(' Train loss:', total_loss / n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' Who the hell are you to judge ?  Must be one of those who casts the first stone .  Your NWO will fail period .  You just do not know it yet ,  opps ,  now you do .  ': 0.55481035}\n",
      "\n",
      "{' parker had more points ,  assists ,  rebounds then westbrook despite taking less shots .  How did Westbrook do a terrific job on parker ?  lol Thunder fans are not very bright ': 0.00075073628}\n",
      "\n",
      "{' Non ho detto questo ,  quindi non inventare cose .  Anzi ,  ho detto che delle scie mi interessa poco e vanno verificate ,  quindi di che parliamo ?  Insunto ,  chi e contro il nucleare ,  esperti o meno che siano ,  sono tutti scemi ?  ': 0.0010902315}\n"
     ]
    }
   ],
   "source": [
    "print(predict_analysis(test_insults, test_predicted, np.array(test_probability)[:,1], 'fp', df_test)[50])\n",
    "print('')\n",
    "print(predict_analysis(test_insults, test_predicted, np.array(test_probability)[:,1], 'fn', df_test)[10])\n",
    "print('')\n",
    "print(predict_analysis(test_insults, test_predicted, np.array(test_probability)[:,1], 'tp', df_test)[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* attention visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "['isalill are you fucking Norwegian cow face', 'Fuck norway bitch', 'Moo moo']\n",
      "Insult:  1\n",
      "\n",
      "1 : \u001b[38;5;232m\u001b[48;5;195m isalill are you fucking Norwegian cow face \u001b[1m\n",
      "2 : \u001b[38;5;232m\u001b[48;5;81m Fuck norway bitch \u001b[1m\n",
      "3 : \u001b[38;5;232m\u001b[48;5;195m Moo moo \u001b[1m\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "['man anytime somebody makes a comment', 'especially one that accurately describes something', 'there is gotta be one motherfucker to come in an shake things up huh', 'shut your god damn mouth you fucking DIVA BOY']\n",
      "Insult:  1\n",
      "\n",
      "1 : \u001b[38;5;232m man anytime somebody makes a comment \u001b[1m\n",
      "2 : \u001b[38;5;232m especially one that accurately describes something \u001b[1m\n",
      "3 : \u001b[38;5;232m there is gotta be one motherfucker to come in an shake things up huh \u001b[1m\n",
      "4 : \u001b[38;5;232m\u001b[48;5;75m shut your god damn mouth you fucking DIVA BOY \u001b[1m\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "['Hi circum I see no problem with that of dogs and foxes in England', 'As nobody seems to see any problem', 'as if ti could ahahahahah', 'when the lions torn to shreds zebras and other animals for example', 'It is Nature', 'But the pathetic twisted post modernist politically correct idiots want to regulate everything', 'They can regulate their fat a*ses']\n",
      "Insult:  0\n",
      "\n",
      "1 : \u001b[38;5;232m Hi circum I see no problem with that of dogs and foxes in England \u001b[1m\n",
      "2 : \u001b[38;5;232m As nobody seems to see any problem \u001b[1m\n",
      "3 : \u001b[38;5;232m as if ti could ahahahahah \u001b[1m\n",
      "4 : \u001b[38;5;232m when the lions torn to shreds zebras and other animals for example \u001b[1m\n",
      "5 : \u001b[38;5;232m It is Nature \u001b[1m\n",
      "6 : \u001b[38;5;232m\u001b[48;5;123m But the pathetic twisted post modernist politically correct idiots want to regulate everything \u001b[1m\n",
      "7 : \u001b[38;5;232m\u001b[48;5;51m They can regulate their fat a*ses \u001b[1m\n",
      "----------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from colored import fg, bg, attr\n",
    "\n",
    "\n",
    "def attentionprint(comments_pd, A, k):\n",
    "    attention = A[k]\n",
    "    input_sentences = eval(comments_pd[\"Comment constructed\"][k]) \n",
    "    print(input_sentences)\n",
    "    print('Insult: ', comments_pd['Insult'][k])\n",
    "    print('')\n",
    "    for i,sentence in enumerate(input_sentences):\n",
    "        if 0 <= attention[i] < 0.075 :\n",
    "            print ('%s :' % (i+1),  '%s %s %s' % (fg(232), sentence, attr('bold')))\n",
    "        else:\n",
    "            if 0.075<= attention[i] < 0.15:\n",
    "                color = 195\n",
    "            elif 0.15<= attention[i] < 0.3:\n",
    "                color = 159\n",
    "            elif 0.3<= attention[i] < 0.475:\n",
    "                color = 123\n",
    "            elif 0.475<= attention[i] < 0.625:\n",
    "                color = 51\n",
    "            elif 0.625<= attention[i] < 0.75:\n",
    "                color =14\n",
    "            elif 0.75<= attention[i] < 0.875:  \n",
    "                color = 81\n",
    "            elif 0.875 <= attention[i] <= 1:\n",
    "                color = 75\n",
    "            print ('%s :' % (i+1),  '%s%s %s %s' % (fg(232), bg(color), sentence, attr('bold')))\n",
    "    print('----------------------------------------------------------------------------------------------------------------------------------')\n",
    "    print('')\n",
    "k = 1097\n",
    "\n",
    "print('----------------------------------------------------------------------------------------------------------------------------------')\n",
    "print('')\n",
    "attentionprint(df_test, A, 97)\n",
    "attentionprint(df_test, A, 197)\n",
    "attentionprint(df_test, A, 497)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "97,197,397,497"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
